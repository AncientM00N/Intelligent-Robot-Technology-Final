"""
æ¨¡å‹è¯„ä¼°ä¸å¯¹æ¯”è„šæœ¬
ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿›è¡Œæµ‹è¯•è¯„ä¼°å¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
"""

import os
import argparse
import numpy as np
from tqdm import tqdm
from typing import Dict, Tuple
import json

from config import BENCHMARK_CONFIG
from environment.frozen_lake_wrapper import FrozenLakeWrapper
from agents.q_learning_agent import QLearningAgent
from agents.sarsa import SARSAAgent
from agents.dqn_agent import DQNAgent


def evaluate_agent(agent, 
                   env: FrozenLakeWrapper, 
                   n_episodes: int = 100,
                   max_steps: int = 100,
                   verbose: bool = True) -> Dict:
    """
    è¯„ä¼°å•ä¸ª Agent
    
    Args:
        agent: è¦è¯„ä¼°çš„æ™ºèƒ½ä½“
        env: ç¯å¢ƒ
        n_episodes: æµ‹è¯•å›åˆæ•°
        max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°
        verbose: æ˜¯å¦æ‰“å°è¿›åº¦
        
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    success_count = 0
    rewards = []
    steps = []
    trajectories = []
    
    iterator = tqdm(range(n_episodes), desc="è¯„ä¼°ä¸­") if verbose else range(n_episodes)
    
    for _ in iterator:
        state, _ = env.reset()
        total_reward = 0
        trajectory = [state]
        
        for step in range(max_steps):
            # è´ªå¿ƒç­–ç•¥ï¼ˆä¸æ¢ç´¢ï¼‰
            action = agent.select_action(state, training=False)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            total_reward += reward
            state = next_state
            trajectory.append(state)
            
            if done:
                break
        
        rewards.append(total_reward)
        steps.append(step + 1)
        trajectories.append(trajectory)
        
        if total_reward > 0:
            success_count += 1
    
    return {
        'success_count': success_count,
        'success_rate': success_count / n_episodes,
        'avg_reward': np.mean(rewards),
        'std_reward': np.std(rewards),
        'avg_steps': np.mean(steps),
        'std_steps': np.std(steps),
        'min_steps': np.min(steps),
        'max_steps': np.max(steps),
        'rewards': rewards,
        'steps': steps,
        'trajectories': trajectories,
    }


def load_agents(model_dir: str, map_size: str, env: FrozenLakeWrapper) -> Dict:
    """
    åŠ è½½æ‰€æœ‰ç®—æ³•çš„æ¨¡å‹
    
    Args:
        model_dir: æ¨¡å‹ç›®å½•
        map_size: åœ°å›¾å¤§å°
        env: ç¯å¢ƒå®ä¾‹ï¼ˆç”¨äºè·å–çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ï¼‰
        
    Returns:
        {ç®—æ³•å: agentå¯¹è±¡} å­—å…¸
    """
    agents = {}
    
    # Q-Learning
    q_learning_path = os.path.join(model_dir, 'q_learning.npz')
    if os.path.exists(q_learning_path):
        print(f"åŠ è½½ Q-Learning æ¨¡å‹: {q_learning_path}")
        agent = QLearningAgent(n_states=env.n_states, n_actions=env.n_actions)
        agent.load(q_learning_path)
        agents['Q-Learning'] = agent
    else:
        print(f"è­¦å‘Š: æœªæ‰¾åˆ° Q-Learning æ¨¡å‹: {q_learning_path}")
    
    # SARSA
    sarsa_path = os.path.join(model_dir, 'sarsa.npz')
    if os.path.exists(sarsa_path):
        print(f"åŠ è½½ SARSA æ¨¡å‹: {sarsa_path}")
        agent = SARSAAgent(n_states=env.n_states, n_actions=env.n_actions)
        agent.load(sarsa_path)
        agents['SARSA'] = agent
    else:
        print(f"è­¦å‘Š: æœªæ‰¾åˆ° SARSA æ¨¡å‹: {sarsa_path}")
    
    # DQN
    dqn_path = os.path.join(model_dir, 'dqn.pth')
    if os.path.exists(dqn_path):
        print(f"åŠ è½½ DQN æ¨¡å‹: {dqn_path}")
        agent = DQNAgent(n_states=env.n_states, n_actions=env.n_actions, use_double_dqn=False)
        agent.load(dqn_path)
        agents['DQN'] = agent
    else:
        print(f"è­¦å‘Š: æœªæ‰¾åˆ° DQN æ¨¡å‹: {dqn_path}")
    
    # DDQN
    ddqn_path = os.path.join(model_dir, 'ddqn.pth')
    if os.path.exists(ddqn_path):
        print(f"åŠ è½½ DDQN æ¨¡å‹: {ddqn_path}")
        agent = DQNAgent(n_states=env.n_states, n_actions=env.n_actions, use_double_dqn=True)
        agent.load(ddqn_path)
        agents['DDQN'] = agent
    else:
        print(f"è­¦å‘Š: æœªæ‰¾åˆ° DDQN æ¨¡å‹: {ddqn_path}")
    
    print(f"\næˆåŠŸåŠ è½½ {len(agents)} ä¸ªæ¨¡å‹\n")
    return agents


def generate_comparison_report(results: Dict, 
                               map_size: str, 
                               mode: str,
                               n_episodes: int) -> str:
    """
    ç”Ÿæˆ Markdown æ ¼å¼çš„å¯¹æ¯”æŠ¥å‘Š
    
    Args:
        results: {ç®—æ³•å: è¯„ä¼°ç»“æœ} å­—å…¸
        map_size: åœ°å›¾å¤§å°
        mode: ç¯å¢ƒæ¨¡å¼
        n_episodes: æµ‹è¯•å›åˆæ•°
        
    Returns:
        Markdown æ ¼å¼æŠ¥å‘Šæ–‡æœ¬
    """
    report_lines = [
        "# ç®—æ³•è¯„ä¼°å¯¹æ¯”æŠ¥å‘Š\n",
        f"**åœ°å›¾å¤§å°**: {map_size}",
        f"**ç¯å¢ƒæ¨¡å¼**: {mode}",
        f"**æµ‹è¯•å›åˆæ•°**: {n_episodes}\n",
        "---\n",
        "## ğŸ“Š æ€§èƒ½å¯¹æ¯”\n",
        "| ç®—æ³• | æˆåŠŸç‡ | å¹³å‡å¥–åŠ± | å¹³å‡æ­¥æ•° | æœ€å°‘æ­¥æ•° | æœ€å¤šæ­¥æ•° |",
        "|------|--------|----------|----------|----------|----------|",
    ]
    
    # æŒ‰æˆåŠŸç‡æ’åº
    sorted_results = sorted(results.items(), key=lambda x: x[1]['success_rate'], reverse=True)
    
    for algo_name, result in sorted_results:
        report_lines.append(
            f"| {algo_name} | "
            f"{result['success_rate']:.2%} | "
            f"{result['avg_reward']:.3f} Â± {result['std_reward']:.3f} | "
            f"{result['avg_steps']:.1f} Â± {result['std_steps']:.1f} | "
            f"{result['min_steps']} | "
            f"{result['max_steps']} |"
        )
    
    report_lines.append("\n---\n")
    report_lines.append("## ğŸ† æœ€ä¼˜ç®—æ³•\n")
    
    # æ‰¾å‡ºæœ€ä¼˜ç®—æ³•
    best_success = max(results.items(), key=lambda x: x[1]['success_rate'])
    best_reward = max(results.items(), key=lambda x: x[1]['avg_reward'])
    best_steps = min(results.items(), key=lambda x: x[1]['avg_steps'])
    
    report_lines.append(f"- **æœ€é«˜æˆåŠŸç‡**: {best_success[0]} ({best_success[1]['success_rate']:.2%})")
    report_lines.append(f"- **æœ€é«˜å¹³å‡å¥–åŠ±**: {best_reward[0]} ({best_reward[1]['avg_reward']:.3f})")
    report_lines.append(f"- **æœ€å°‘å¹³å‡æ­¥æ•°**: {best_steps[0]} ({best_steps[1]['avg_steps']:.1f} æ­¥)")
    
    report_lines.append("\n---\n")
    report_lines.append("## ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡\n")
    
    for algo_name, result in sorted_results:
        report_lines.append(f"\n### {algo_name}\n")
        report_lines.append(f"- æˆåŠŸæ¬¡æ•°: {result['success_count']}/{n_episodes}")
        report_lines.append(f"- æˆåŠŸç‡: {result['success_rate']:.2%}")
        report_lines.append(f"- å¹³å‡å¥–åŠ±: {result['avg_reward']:.3f} (æ ‡å‡†å·®: {result['std_reward']:.3f})")
        report_lines.append(f"- å¹³å‡æ­¥æ•°: {result['avg_steps']:.1f} (æ ‡å‡†å·®: {result['std_steps']:.1f})")
        report_lines.append(f"- æ­¥æ•°èŒƒå›´: [{result['min_steps']}, {result['max_steps']}]")
    
    return "\n".join(report_lines)


def save_results(results: Dict, 
                 save_path: str):
    """
    ä¿å­˜è¯„ä¼°ç»“æœä¸º JSON æ ¼å¼
    
    Args:
        results: è¯„ä¼°ç»“æœå­—å…¸
        save_path: ä¿å­˜è·¯å¾„
    """
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    # è½¬æ¢ numpy ç±»å‹ä¸º Python åŸç”Ÿç±»å‹
    json_results = {}
    for algo_name, result in results.items():
        json_results[algo_name] = {
            'success_count': int(result['success_count']),
            'success_rate': float(result['success_rate']),
            'avg_reward': float(result['avg_reward']),
            'std_reward': float(result['std_reward']),
            'avg_steps': float(result['avg_steps']),
            'std_steps': float(result['std_steps']),
            'min_steps': int(result['min_steps']),
            'max_steps': int(result['max_steps']),
            'rewards': [float(r) for r in result['rewards']],
            'steps': [int(s) for s in result['steps']],
        }
    
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(json_results, f, indent=2, ensure_ascii=False)
    
    print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜: {save_path}")


def visualize_sample_trajectories(results: Dict,
                                  env: FrozenLakeWrapper,
                                  n_samples: int = 3):
    """
    å¯è§†åŒ–æ ·ä¾‹è½¨è¿¹
    
    Args:
        results: è¯„ä¼°ç»“æœ
        env: ç¯å¢ƒ
        n_samples: å±•ç¤ºæ ·ä¾‹æ•°
    """
    print("\n" + "="*80)
    print("æ ·ä¾‹è½¨è¿¹å¯è§†åŒ–")
    print("="*80)
    
    for algo_name, result in results.items():
        print(f"\n### {algo_name} ###")
        
        # æ‰¾å‡ºæˆåŠŸçš„è½¨è¿¹
        successful_indices = [i for i, r in enumerate(result['rewards']) if r > 0]
        
        if not successful_indices:
            print("  æ²¡æœ‰æˆåŠŸçš„è½¨è¿¹")
            continue
        
        # éšæœºé€‰æ‹©å‡ ä¸ªæˆåŠŸè½¨è¿¹
        sample_indices = np.random.choice(successful_indices, 
                                         min(n_samples, len(successful_indices)), 
                                         replace=False)
        
        for idx in sample_indices:
            trajectory = result['trajectories'][idx]
            steps_taken = result['steps'][idx]
            
            print(f"\n  æ ·ä¾‹ {idx+1} (æ­¥æ•°: {steps_taken}):")
            print(f"  è½¨è¿¹: ", end="")
            
            for state in trajectory:
                row, col = env.state_to_coord(state)
                print(f"({row},{col})", end=" -> " if state != trajectory[-1] else "")
            
            print(" [åˆ°è¾¾ç›®æ ‡]")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ¨¡å‹è¯„ä¼°ä¸å¯¹æ¯”è„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è¯„ä¼° 8x8 éšæœºæ¨¡å¼çš„æ¨¡å‹
  python evaluate_comparison.py --map_size 8x8 --mode stochastic
  
  # è¯„ä¼° 4x4 ç¡®å®šæ€§æ¨¡å¼çš„æ¨¡å‹ï¼ˆ200æ¬¡æµ‹è¯•ï¼‰
  python evaluate_comparison.py --map_size 4x4 --mode deterministic --episodes 200
  
  # æŒ‡å®šæ¨¡å‹ç›®å½•
  python evaluate_comparison.py --map_size 8x8 --model_dir models/my_benchmark
        """
    )
    
    parser.add_argument('--map_size', type=str, default='8x8',
                       choices=['4x4', '8x8'],
                       help='åœ°å›¾å¤§å° (é»˜è®¤: 8x8)')
    parser.add_argument('--mode', type=str, default='stochastic',
                       choices=['deterministic', 'stochastic'],
                       help='ç¯å¢ƒæ¨¡å¼ (é»˜è®¤: stochastic)')
    parser.add_argument('--model_dir', type=str,
                       help='æ¨¡å‹ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨ Benchmark ç›®å½•ï¼‰')
    parser.add_argument('--episodes', type=int,
                       help='æµ‹è¯•å›åˆæ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--max_steps', type=int,
                       help='æ¯å›åˆæœ€å¤§æ­¥æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--save_dir', type=str,
                       help='ç»“æœä¿å­˜ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨ Benchmark ç›®å½•ï¼‰')
    parser.add_argument('--show_trajectories', action='store_true',
                       help='æ˜¾ç¤ºæ ·ä¾‹è½¨è¿¹')
    
    args = parser.parse_args()
    
    # è®¾ç½®è·¯å¾„
    if args.model_dir:
        model_dir = args.model_dir
    else:
        model_dir = BENCHMARK_CONFIG['model_dir_template'].format(
            map_size=args.map_size, mode=args.mode
        )
    
    if args.save_dir:
        save_dir = args.save_dir
    else:
        save_dir = BENCHMARK_CONFIG['result_dir_template'].format(
            map_size=args.map_size, mode=args.mode
        )
    
    # è¯„ä¼°å‚æ•°
    n_episodes = args.episodes or BENCHMARK_CONFIG['eval_episodes']
    max_steps = args.max_steps or BENCHMARK_CONFIG['eval_max_steps']
    
    print("\n" + "="*80)
    print("æ¨¡å‹è¯„ä¼°ä¸å¯¹æ¯”")
    print("="*80)
    print(f"åœ°å›¾å¤§å°: {args.map_size}")
    print(f"ç¯å¢ƒæ¨¡å¼: {args.mode}")
    print(f"æ¨¡å‹ç›®å½•: {model_dir}")
    print(f"æµ‹è¯•å›åˆæ•°: {n_episodes}")
    print(f"æœ€å¤§æ­¥æ•°: {max_steps}")
    print("="*80)
    
    # åˆ›å»ºç¯å¢ƒ
    is_slippery = (args.mode == 'stochastic')
    env = FrozenLakeWrapper(map_size=args.map_size, is_slippery=is_slippery)
    
    print(f"\nç¯å¢ƒä¿¡æ¯:")
    env.print_map()
    
    # åŠ è½½æ¨¡å‹
    print(f"\n{'='*80}")
    print("åŠ è½½æ¨¡å‹...")
    print(f"{'='*80}")
    
    agents = load_agents(model_dir, args.map_size, env)
    
    if not agents:
        print("é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•æ¨¡å‹ï¼")
        return
    
    # è¯„ä¼°æ‰€æœ‰æ¨¡å‹
    print(f"\n{'='*80}")
    print(f"å¼€å§‹è¯„ä¼° ({n_episodes} å›åˆ)...")
    print(f"{'='*80}\n")
    
    results = {}
    for algo_name, agent in agents.items():
        print(f"\n--- è¯„ä¼° {algo_name} ---")
        result = evaluate_agent(agent, env, n_episodes, max_steps, verbose=True)
        results[algo_name] = result
        
        print(f"æˆåŠŸç‡: {result['success_rate']:.2%}")
        print(f"å¹³å‡å¥–åŠ±: {result['avg_reward']:.3f}")
        print(f"å¹³å‡æ­¥æ•°: {result['avg_steps']:.1f}")
    
    env.close()
    
    # ç”ŸæˆæŠ¥å‘Š
    print(f"\n{'='*80}")
    print("ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
    print(f"{'='*80}")
    
    os.makedirs(save_dir, exist_ok=True)
    
    report = generate_comparison_report(results, args.map_size, args.mode, n_episodes)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = os.path.join(save_dir, 'evaluation_report.md')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # ä¿å­˜ JSON ç»“æœ
    json_path = os.path.join(save_dir, 'evaluation_results.json')
    save_results(results, json_path)
    
    # æ‰“å°æŠ¥å‘Š
    print("\n" + report)
    
    # å¯è§†åŒ–è½¨è¿¹
    if args.show_trajectories:
        visualize_sample_trajectories(results, env, n_samples=3)
    
    print(f"\n{'='*80}")
    print("è¯„ä¼°å®Œæˆ!")
    print(f"{'='*80}")
    print(f"ç»“æœä¿å­˜åœ¨: {save_dir}")


if __name__ == '__main__':
    main()
