"""
è¯„ä¼°è„šæœ¬
ç”¨äºè¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹æ€§èƒ½
"""

import os
import argparse
import numpy as np
from tqdm import tqdm
from typing import Tuple, List

from config import ENV_CONFIG
from environment.frozen_lake_wrapper import FrozenLakeWrapper
from agents.q_learning_agent import QLearningAgent
from agents.dqn_agent import DQNAgent
from utils.visualization import Visualizer, plot_q_table_heatmap


def evaluate_agent(agent, env: FrozenLakeWrapper, 
                  num_episodes: int = 1000,
                  max_steps: int = 100,
                  verbose: bool = True) -> Tuple[float, float, List[float]]:
    """
    è¯„ä¼° Agent æ€§èƒ½
    
    Args:
        agent: è®­ç»ƒå¥½çš„ Agent
        env: FrozenLake ç¯å¢ƒ
        num_episodes: è¯„ä¼°å›åˆæ•°
        max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°
        verbose: æ˜¯å¦æ‰“å°è¿›åº¦
        
    Returns:
        (success_rate, avg_reward, rewards): æˆåŠŸç‡ï¼Œå¹³å‡å¥–åŠ±ï¼Œå¥–åŠ±åˆ—è¡¨
    """
    rewards = []
    successes = 0
    
    iterator = tqdm(range(num_episodes), desc="è¯„ä¼°ä¸­") if verbose else range(num_episodes)
    
    for episode in iterator:
        state, _ = env.reset()
        total_reward = 0
        
        for step in range(max_steps):
            # ä½¿ç”¨è´ªå¿ƒç­–ç•¥ï¼ˆä¸æ¢ç´¢ï¼‰
            action = agent.select_action(state, training=False)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            total_reward += reward
            state = next_state
            
            if done:
                break
        
        rewards.append(total_reward)
        if total_reward > 0:
            successes += 1
    
    success_rate = successes / num_episodes
    avg_reward = np.mean(rewards)
    
    return success_rate, avg_reward, rewards


def compare_models(env: FrozenLakeWrapper, 
                  model_dir: str = 'models',
                  num_episodes: int = 1000):
    """
    æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½
    
    Args:
        env: FrozenLake ç¯å¢ƒ
        model_dir: æ¨¡å‹ç›®å½•
        num_episodes: è¯„ä¼°å›åˆæ•°
    """
    results = {}
    
    print(f"\n{'='*60}")
    print("æ¨¡å‹æ€§èƒ½è¯„ä¼°")
    print(f"{'='*60}")
    print(f"è¯„ä¼°å›åˆæ•°: {num_episodes}")
    print(f"ç¯å¢ƒ: {env.grid_size}x{env.grid_size}, "
          f"{'Stochastic' if env.is_slippery else 'Deterministic'}")
    print(f"{'='*60}\n")
    
    # è¯„ä¼° Q-Learning
    q_learning_path = os.path.join(model_dir, 'q_learning.npz')
    if os.path.exists(q_learning_path):
        print("è¯„ä¼° Q-Learning...")
        q_agent = QLearningAgent(env.n_states, env.n_actions)
        q_agent.load(q_learning_path)
        
        success_rate, avg_reward, rewards = evaluate_agent(q_agent, env, num_episodes)
        results['Q-Learning'] = {
            'success_rate': success_rate,
            'avg_reward': avg_reward,
            'rewards': rewards
        }
        print(f"  æˆåŠŸç‡: {success_rate:.2%}")
        print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.4f}\n")
    
    # è¯„ä¼° DQN
    dqn_path = os.path.join(model_dir, 'dqn.pth')
    if os.path.exists(dqn_path):
        print("è¯„ä¼° DQN...")
        dqn_agent = DQNAgent(env.n_states, env.n_actions)
        dqn_agent.load(dqn_path)
        
        success_rate, avg_reward, rewards = evaluate_agent(dqn_agent, env, num_episodes)
        results['DQN'] = {
            'success_rate': success_rate,
            'avg_reward': avg_reward,
            'rewards': rewards
        }
        print(f"  æˆåŠŸç‡: {success_rate:.2%}")
        print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.4f}\n")
    
    # è¯„ä¼° Double DQN
    ddqn_path = os.path.join(model_dir, 'ddqn.pth')
    if os.path.exists(ddqn_path):
        print("è¯„ä¼° Double DQN...")
        ddqn_agent = DQNAgent(env.n_states, env.n_actions)
        ddqn_agent.load(ddqn_path)
        
        success_rate, avg_reward, rewards = evaluate_agent(ddqn_agent, env, num_episodes)
        results['Double DQN'] = {
            'success_rate': success_rate,
            'avg_reward': avg_reward,
            'rewards': rewards
        }
        print(f"  æˆåŠŸç‡: {success_rate:.2%}")
        print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.4f}\n")
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    if results:
        print("=" * 60)
        print("è¯„ä¼°ç»“æœå¯¹æ¯”")
        print("=" * 60)
        print(f"{'ç®—æ³•':<15} {'æˆåŠŸç‡':<15} {'å¹³å‡å¥–åŠ±':<15}")
        print("-" * 45)
        
        for name, data in results.items():
            print(f"{name:<15} {data['success_rate']:.2%}{'':<9} {data['avg_reward']:.4f}")
        
        print("=" * 60)
    
    return results


def visualize_policy(agent, env: FrozenLakeWrapper, save_dir: str = 'plots'):
    """
    å¯è§†åŒ– Agent çš„ç­–ç•¥
    
    Args:
        agent: è®­ç»ƒå¥½çš„ Agent
        env: FrozenLake ç¯å¢ƒ
        save_dir: å›¾è¡¨ä¿å­˜ç›®å½•
    """
    os.makedirs(save_dir, exist_ok=True)
    
    # è·å–ç­–ç•¥
    policy = agent.get_policy()
    
    # åŠ¨ä½œç¬¦å·
    action_symbols = {0: 'â†', 1: 'â†“', 2: 'â†’', 3: 'â†‘'}
    
    print("\n=== å­¦ä¹ åˆ°çš„ç­–ç•¥ ===")
    print("+" + "---+" * env.grid_size)
    
    for row in range(env.grid_size):
        line = "|"
        for col in range(env.grid_size):
            state = row * env.grid_size + col
            cell_type = env.get_cell_type(state)
            
            if cell_type == 'G':
                symbol = ' G '
            elif cell_type == 'H':
                symbol = ' H '
            else:
                action = policy[state]
                symbol = f' {action_symbols[action]} '
            
            line += symbol + '|'
        print(line)
        print("+" + "---+" * env.grid_size)
    
    # å¦‚æœæ˜¯ Q-Learningï¼Œç»˜åˆ¶ Q-Table çƒ­åŠ›å›¾
    if hasattr(agent, 'q_table'):
        plot_q_table_heatmap(
            agent.q_table,
            grid_size=env.grid_size,
            title='Q-Learning Q-Table çƒ­åŠ›å›¾',
            save_path=os.path.join(save_dir, 'q_table_heatmap.png'),
            show=True
        )


def run_single_episode(agent, env: FrozenLakeWrapper, 
                       verbose: bool = True,
                       step_delay: float = 0.5):
    """
    è¿è¡Œå•ä¸ªå›åˆå¹¶è¯¦ç»†å±•ç¤ºè¿‡ç¨‹
    
    Args:
        agent: è®­ç»ƒå¥½çš„ Agent
        env: FrozenLake ç¯å¢ƒ
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        step_delay: æ¯æ­¥å»¶è¿Ÿ
    """
    import time
    
    state, _ = env.reset()
    
    print("\n" + "=" * 50)
    print("å•å›åˆæ¼”ç¤º")
    print("=" * 50)
    
    path = [env.state_to_coord(state)]
    actions = []
    
    env.print_map(state)
    
    for step in range(100):
        action = agent.select_action(state, training=False)
        actions.append(action)
        
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
        row, col = env.state_to_coord(next_state)
        path.append((row, col))
        
        if verbose:
            action_name = ['å·¦', 'ä¸‹', 'å³', 'ä¸Š'][action]
            print(f"\næ­¥éª¤ {step + 1}: åŠ¨ä½œ = {action_name}")
            print(f"çŠ¶æ€: {state} -> {next_state}")
            print(f"ä½ç½®: {env.state_to_coord(state)} -> ({row}, {col})")
            print(f"å¥–åŠ±: {reward}")
            env.print_map(next_state)
        
        state = next_state
        
        if done:
            if reward > 0:
                print("\nğŸ‰ æˆåŠŸåˆ°è¾¾ç›®æ ‡!")
            else:
                print("\nğŸ’€ æ‰å…¥å†°çªŸçª¿!")
            break
        
        time.sleep(step_delay)
    
    # æ‰“å°è·¯å¾„æ‘˜è¦
    print("\nè·¯å¾„æ‘˜è¦:")
    print(" -> ".join([f"({r},{c})" for r, c in path]))
    print(f"æ€»æ­¥æ•°: {len(actions)}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='FrozenLake æ¨¡å‹è¯„ä¼°')
    parser.add_argument('--model', type=str, default=None,
                       help='æ¨¡å‹è·¯å¾„ï¼ˆå¦‚ä¸æŒ‡å®šåˆ™è¯„ä¼°æ‰€æœ‰æ¨¡å‹ï¼‰')
    parser.add_argument('--model_dir', type=str, default='models',
                       help='æ¨¡å‹ç›®å½•')
    parser.add_argument('--map_size', type=str, default='4x4',
                       choices=['4x4', '8x8'],
                       help='åœ°å›¾å¤§å°')
    parser.add_argument('--slippery', action='store_true', default=True,
                       help='å¯ç”¨éšæœºæ¨¡å¼')
    parser.add_argument('--no-slippery', action='store_false', dest='slippery',
                       help='ä½¿ç”¨ç¡®å®šæ€§æ¨¡å¼')
    parser.add_argument('--episodes', type=int, default=1000,
                       help='è¯„ä¼°å›åˆæ•°')
    parser.add_argument('--demo', action='store_true',
                       help='è¿è¡Œå•å›åˆæ¼”ç¤º')
    parser.add_argument('--visualize', action='store_true',
                       help='å¯è§†åŒ–ç­–ç•¥')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç¯å¢ƒ
    env = FrozenLakeWrapper(
        map_size=args.map_size,
        is_slippery=args.slippery
    )
    
    print(f"\nç¯å¢ƒé…ç½®:")
    print(f"  åœ°å›¾å¤§å°: {args.map_size}")
    print(f"  æ¨¡å¼: {'Stochastic (éšæœº)' if args.slippery else 'Deterministic (ç¡®å®šæ€§)'}")
    
    if args.model:
        # è¯„ä¼°æŒ‡å®šæ¨¡å‹
        if args.model.endswith('.npz'):
            agent = QLearningAgent(env.n_states, env.n_actions)
        else:
            agent = DQNAgent(env.n_states, env.n_actions)
        
        agent.load(args.model)
        
        if args.demo:
            run_single_episode(agent, env)
        elif args.visualize:
            visualize_policy(agent, env)
        else:
            success_rate, avg_reward, _ = evaluate_agent(agent, env, args.episodes)
            print(f"\nè¯„ä¼°ç»“æœ:")
            print(f"  æˆåŠŸç‡: {success_rate:.2%}")
            print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.4f}")
    else:
        # è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        results = compare_models(env, args.model_dir, args.episodes)
        
        if args.visualize and os.path.exists(os.path.join(args.model_dir, 'q_learning.npz')):
            q_agent = QLearningAgent(env.n_states, env.n_actions)
            q_agent.load(os.path.join(args.model_dir, 'q_learning.npz'))
            visualize_policy(q_agent, env)
    
    env.close()


if __name__ == '__main__':
    main()

