================================================================================
强化学习算法 Benchmark 实验 - 参数配置汇总
================================================================================

实验时间: 2026-01-15 15:45:20
地图大小: 4x4
环境模式: deterministic (确定性/无风)
算法数量: 4

================================================================================

【算法 1/4】 Q-Learning
--------------------------------------------------------------------------------
算法类型: 表格法 (Tabular Method)
值函数表示: Q-Table

核心参数:
  训练回合数 (num_episodes):         5000
  每回合最大步数 (max_steps):          100
  学习率 (learning_rate):             0.8
  折扣因子 (discount_factor):        0.95

探索策略 (ε-greedy):
  初始探索率 (epsilon_start):         1.0
  最终探索率 (epsilon_end):          0.01
  探索率衰减 (epsilon_decay):       0.997

================================================================================

【算法 2/4】 SARSA
--------------------------------------------------------------------------------
算法类型: 表格法 (Tabular Method)
值函数表示: Q-Table

核心参数:
  训练回合数 (num_episodes):         5000
  每回合最大步数 (max_steps):          100
  学习率 (learning_rate):             0.8
  折扣因子 (discount_factor):       0.995

探索策略 (ε-greedy):
  初始探索率 (epsilon_start):         1.0
  最终探索率 (epsilon_end):          0.01
  探索率衰减 (epsilon_decay):       0.997

================================================================================

【算法 3/4】 DQN
--------------------------------------------------------------------------------
算法类型: 深度学习法 (Deep Reinforcement Learning)
值函数表示: 神经网络

核心参数:
  训练回合数 (num_episodes):         5000
  每回合最大步数 (max_steps):          100
  学习率 (learning_rate):           0.001
  折扣因子 (discount_factor):        0.99

探索策略 (ε-greedy):
  初始探索率 (epsilon_start):         1.0
  最终探索率 (epsilon_end):          0.01
  探索率衰减 (epsilon_decay):       0.995

深度学习特有参数:
  批量大小 (batch_size):               64
  经验回放缓冲区 (buffer_size):     10000
  目标网络更新频率 (target_update_freq):  100
  隐藏层大小 (hidden_size):            64
  Double DQN:                    True

================================================================================

【算法 4/4】 DDQN
--------------------------------------------------------------------------------
算法类型: 深度学习法 (Deep Reinforcement Learning)
值函数表示: 神经网络

核心参数:
  训练回合数 (num_episodes):         5000
  每回合最大步数 (max_steps):          100
  学习率 (learning_rate):           0.001
  折扣因子 (discount_factor):        0.99

探索策略 (ε-greedy):
  初始探索率 (epsilon_start):         1.0
  最终探索率 (epsilon_end):          0.01
  探索率衰减 (epsilon_decay):       0.995

深度学习特有参数:
  批量大小 (batch_size):               64
  经验回放缓冲区 (buffer_size):     10000
  目标网络更新频率 (target_update_freq):  100
  隐藏层大小 (hidden_size):            64
  Double DQN:                    True

================================================================================

算法对比说明
================================================================================

【表格法 vs 深度学习法】

表格法 (Q-Learning, SARSA):
  优点: 简单直观，理论保证收敛（充分探索下）
  缺点: 无泛化能力，每个状态必须单独学习
  适用: 小状态空间 (<100 个状态)

深度学习法 (DQN, DDQN):
  优点: 强大泛化能力，经验回放，目标网络稳定训练
  缺点: 更复杂，超参数敏感
  适用: 大状态空间 (>1000 个状态)

【Q-Learning vs SARSA】

Q-Learning (Off-Policy):
  更新: 使用 max Q(s',a') - 学习最优策略
  特点: 更激进，更适合随机环境

SARSA (On-Policy):
  更新: 使用实际执行的 Q(s',a') - 学习执行策略
  特点: 更保守，在随机环境中可能表现较差

【DQN vs DDQN】

DQN (Nature DQN):
  目标: max Q(s',a'; θ⁻) - 同一网络选择和评估
  问题: 容易过估计 Q 值

DDQN (Double DQN):
  目标: Q(s', argmax Q(s',a;θ); θ⁻) - 解耦选择和评估
  优势: 减少过估计，通常表现更好

================================================================================
配置文件生成时间: 2026-01-15 15:45:20
================================================================================
