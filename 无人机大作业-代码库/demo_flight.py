"""
æ— äººæœºé£è¡Œæ¼”ç¤ºè„šæœ¬

å®ç°å…³é”®åŠŸèƒ½:
1. è¾¹ç•Œæ£€æµ‹ï¼šæ‰§è¡ŒåŠ¨ä½œå‰æ£€æŸ¥æ˜¯å¦ä¼šæ’å¢™
2. é£åŠ›å¹²æ‰°æ¨¡æ‹Ÿï¼šStochastic æ¨¡å¼ä¸‹æ¨¡æ‹Ÿéšæœºæ¼‚ç§»
3. å®æ—¶ä½ç½®æ˜¾ç¤ºï¼šæ‰“å°å½“å‰ç½‘æ ¼åæ ‡
4. å›¾å½¢ç•Œé¢æ¼”ç¤ºï¼šä½¿ç”¨ Gym æ¸²æŸ“åŠ¨ç”»
"""

import gym
import numpy as np
import random
import time
import os
from typing import Optional, Tuple, List

from environment.frozen_lake_wrapper import FrozenLakeWrapper
from agents.q_learning_agent import QLearningAgent
from agents.dqn_agent import DQNAgent
from utils.visualization import GridVisualizer


class FlightDemoController:
    """
    æ— äººæœºé£è¡Œæ¼”ç¤ºæ§åˆ¶å™¨
    
    æ ¸å¿ƒåŠŸèƒ½:
    1. ç»´æŠ¤è¾¹ç•Œæ­¥æ•°å˜é‡ (forward_steps, right_steps, left_steps, back_steps)
    2. æ‰§è¡ŒåŠ¨ä½œå‰è¿›è¡Œè¾¹ç•Œæ£€æµ‹
    3. æ¨¡æ‹Ÿ Stochastic æ¨¡å¼çš„é£åŠ›å¹²æ‰°
    4. å®æ—¶æ˜¾ç¤ºä½ç½®
    """
    
    # åŠ¨ä½œå®šä¹‰ï¼ˆä¸ Gym FrozenLake ä¸€è‡´ï¼‰
    ACTION_LEFT = 0
    ACTION_DOWN = 1   # å‰è¿› (åœ¨ç½‘æ ¼ä¸­å‘ä¸‹)
    ACTION_RIGHT = 2
    ACTION_UP = 3     # åé€€ (åœ¨ç½‘æ ¼ä¸­å‘ä¸Š)
    
    ACTION_NAMES = {0: 'å·¦', 1: 'å‰(ä¸‹)', 2: 'å³', 3: 'å(ä¸Š)'}
    ACTION_SYMBOLS = {0: 'â†', 1: 'â†“', 2: 'â†’', 3: 'â†‘'}
    
    def __init__(self, grid_size: int = 4, 
                 is_stochastic: bool = True,
                 wind_probability: float = 0.333):
        """
        åˆå§‹åŒ–æ¼”ç¤ºæ§åˆ¶å™¨
        
        Args:
            grid_size: ç½‘æ ¼å¤§å° (4 æˆ– 8)
            is_stochastic: æ˜¯å¦ä¸ºéšæœºæ¨¡å¼ï¼ˆæœ‰é£åŠ›å¹²æ‰°ï¼‰
            wind_probability: é£åŠ›å¹²æ‰°æ¦‚ç‡ï¼ˆæ¯ä¸ªåç§»æ–¹å‘çš„æ¦‚ç‡ï¼‰
                - 0.333: å¼ºé£ï¼ˆé»˜è®¤ï¼Œ1/3æ¦‚ç‡åç§»ï¼‰
                - 0.2: ä¸­é£
                - 0.1: å¼±é£
                - 0.0: æ— é£
        """
        self.grid_size = grid_size
        self.is_stochastic = is_stochastic
        self.wind_probability = wind_probability
        
        # å½“å‰ä½ç½®ï¼ˆè¡Œ, åˆ—ï¼‰
        self.current_row = 0
        self.current_col = 0
        
        # è¾¹ç•Œæ­¥æ•°ï¼ˆæ ¹æ®éœ€æ±‚æ–‡æ¡£å®šä¹‰ï¼‰
        self.forward_steps = grid_size - 1
        self.back_steps = 0
        self.left_steps = 0
        self.right_steps = grid_size - 1
        
        # é£è¡Œå†å²
        self.path_history: List[Tuple[int, int]] = []
        self.action_history: List[int] = []
        self.intended_actions: List[int] = []
        
        # ç¯å¢ƒåœ°å›¾æè¿°
        self.map_desc: Optional[np.ndarray] = None
        
        # å¯è§†åŒ–å™¨
        self.visualizer = GridVisualizer(grid_size)
        
    def reset(self, start_row: int = 0, start_col: int = 0):
        """é‡ç½®æ§åˆ¶å™¨åˆ°åˆå§‹ä½ç½®"""
        self.current_row = start_row
        self.current_col = start_col
        self._update_boundary_steps()
        
        self.path_history.clear()
        self.action_history.clear()
        self.intended_actions.clear()
        self.path_history.append((start_row, start_col))
        
        if self.map_desc is not None:
            self.visualizer.set_map(self.map_desc)
        self.visualizer.clear_path()
        self.visualizer.add_position(start_row, start_col)
        
    def set_map(self, desc: np.ndarray):
        """è®¾ç½®åœ°å›¾æè¿°"""
        self.map_desc = desc
        self.visualizer.set_map(desc)
    
    def _update_boundary_steps(self):
        """æ›´æ–°è¾¹ç•Œæ­¥æ•°"""
        self.forward_steps = self.grid_size - 1 - self.current_row
        self.back_steps = self.current_row
        self.left_steps = self.current_col
        self.right_steps = self.grid_size - 1 - self.current_col
    
    def get_boundary_steps(self) -> dict:
        """è·å–å½“å‰è¾¹ç•Œæ­¥æ•°"""
        return {
            'forward': self.forward_steps,
            'back': self.back_steps,
            'left': self.left_steps,
            'right': self.right_steps
        }
    
    def is_valid_action(self, action: int) -> bool:
        """æ£€æŸ¥åŠ¨ä½œæ˜¯å¦æœ‰æ•ˆï¼ˆä¸ä¼šæ’å¢™ï¼‰"""
        if action == self.ACTION_LEFT:
            return self.left_steps > 0
        elif action == self.ACTION_DOWN:
            return self.forward_steps > 0
        elif action == self.ACTION_RIGHT:
            return self.right_steps > 0
        elif action == self.ACTION_UP:
            return self.back_steps > 0
        return False
    
    def get_valid_actions(self) -> List[int]:
        """è·å–æ‰€æœ‰æœ‰æ•ˆåŠ¨ä½œ"""
        return [a for a in range(4) if self.is_valid_action(a)]
    
    def apply_wind_effect(self, intended_action: int) -> Tuple[int, str]:
        """
        åº”ç”¨é£åŠ›å¹²æ‰°æ•ˆæœ
        
        Returns:
            (actual_action, wind_status): å®é™…åŠ¨ä½œå’Œé£åŠ›çŠ¶æ€æè¿°
        """
        if not self.is_stochastic or self.wind_probability <= 0:
            return intended_action, "æ— é£"
        
        rand = random.random()
        
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        # åŸåŠ¨ä½œæ¦‚ç‡ = 1 - 2*wind_probability
        # å·¦åæ¦‚ç‡ = wind_probability
        # å³åæ¦‚ç‡ = wind_probability
        no_wind_prob = 1 - 2 * self.wind_probability
        
        if rand < no_wind_prob:
            return intended_action, "æ— é£"
        elif rand < no_wind_prob + self.wind_probability:
            actual_action = (intended_action - 1) % 4
            return actual_action, "å·¦åé£"
        else:
            actual_action = (intended_action + 1) % 4
            return actual_action, "å³åé£"
    
    def execute_action(self, intended_action: int, 
                       apply_wind: bool = True,
                       verbose: bool = True) -> Tuple[bool, str, int]:
        """
        æ‰§è¡ŒåŠ¨ä½œï¼ˆåŒ…å«è¾¹ç•Œæ£€æµ‹å’Œé£åŠ›å¹²æ‰°ï¼‰
        
        Returns:
            (success, message, actual_action): æ˜¯å¦æˆåŠŸï¼Œæ¶ˆæ¯ï¼Œå®é™…æ‰§è¡Œçš„åŠ¨ä½œ
        """
        self.intended_actions.append(intended_action)
        
        # åº”ç”¨é£åŠ›å¹²æ‰°
        if apply_wind and self.is_stochastic:
            actual_action, wind_status = self.apply_wind_effect(intended_action)
        else:
            actual_action = intended_action
            wind_status = "æ— é£"
        
        if verbose:
            print(f"\n--- æ­¥éª¤ {len(self.action_history) + 1} ---")
            print(f"  å½“å‰ä½ç½®: ({self.current_row}, {self.current_col})")
            print(f"  è¾¹ç•Œ: å‰={self.forward_steps}, å={self.back_steps}, "
                  f"å·¦={self.left_steps}, å³={self.right_steps}")
            print(f"  åŸæ„åŠ¨ä½œ: {self.ACTION_SYMBOLS[intended_action]} {self.ACTION_NAMES[intended_action]}")
            
            if wind_status != "æ— é£":
                print(f"  âš  é£åŠ›å¹²æ‰°! [{wind_status}] -> å®é™…: {self.ACTION_NAMES[actual_action]}")
        
        # è¾¹ç•Œæ£€æµ‹
        if not self.is_valid_action(actual_action):
            if verbose:
                print(f"  âœ— è¾¹ç•Œé˜»æ­¢! åŠ¨ä½œ {self.ACTION_NAMES[actual_action]} ä¼šæ’å¢™")
            self.action_history.append(actual_action)
            return False, "è¾¹ç•Œé˜»æ­¢", actual_action
        
        # æ‰§è¡ŒåŠ¨ä½œ
        old_row, old_col = self.current_row, self.current_col
        
        if actual_action == self.ACTION_LEFT:
            self.current_col -= 1
        elif actual_action == self.ACTION_DOWN:
            self.current_row += 1
        elif actual_action == self.ACTION_RIGHT:
            self.current_col += 1
        elif actual_action == self.ACTION_UP:
            self.current_row -= 1
        
        self._update_boundary_steps()
        self.path_history.append((self.current_row, self.current_col))
        self.action_history.append(actual_action)
        self.visualizer.add_position(self.current_row, self.current_col)
        
        if verbose:
            print(f"  âœ“ ç§»åŠ¨: ({old_row}, {old_col}) -> ({self.current_row}, {self.current_col})")
        
        return True, "æˆåŠŸ", actual_action
    
    def select_valid_action(self, agent, state: int, max_retries: int = 10) -> int:
        """é€‰æ‹©ä¸€ä¸ªæœ‰æ•ˆåŠ¨ä½œ"""
        valid_actions = self.get_valid_actions()
        if not valid_actions:
            raise RuntimeError("æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œ!")
        
        for _ in range(max_retries):
            action = agent.select_action(state, training=False)
            if action in valid_actions:
                return action
        
        return random.choice(valid_actions)
    
    def get_current_state(self) -> int:
        """è·å–å½“å‰çŠ¶æ€ID"""
        return self.current_row * self.grid_size + self.current_col
    
    def get_cell_type(self) -> str:
        """è·å–å½“å‰æ ¼å­ç±»å‹"""
        if self.map_desc is None:
            return 'F'
        return self.map_desc[self.current_row, self.current_col]
    
    def is_terminal(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç»ˆæ­¢çŠ¶æ€"""
        return self.get_cell_type() in ['G', 'H']
    
    def is_goal(self) -> bool:
        return self.get_cell_type() == 'G'
    
    def is_hole(self) -> bool:
        return self.get_cell_type() == 'H'
    
    def print_current_state(self):
        """æ‰“å°å½“å‰çŠ¶æ€"""
        if self.map_desc is not None:
            self.visualizer.print_grid(
                (self.current_row, self.current_col),
                show_path=True
            )
    
    def print_summary(self):
        """æ‰“å°é£è¡Œæ‘˜è¦"""
        print("\n" + "=" * 50)
        print("é£è¡Œæ‘˜è¦")
        print("=" * 50)
        print(f"æ€»æ­¥æ•°: {len(self.action_history)}")
        print(f"æœ€ç»ˆä½ç½®: ({self.current_row}, {self.current_col})")
        
        result = 'åˆ°è¾¾ç›®æ ‡!' if self.is_goal() else 'æ‰å…¥å†°çªŸçª¿!' if self.is_hole() else 'æœªå®Œæˆ'
        print(f"ç»“æœ: {result}")
        
        # ç»Ÿè®¡é£åŠ›å½±å“
        wind_count = sum(1 for i, a in enumerate(self.intended_actions) 
                        if i < len(self.action_history) and a != self.action_history[i])
        print(f"é£åŠ›åç§»æ¬¡æ•°: {wind_count}/{len(self.action_history)}")
        
        print("\nè·¯å¾„: ", end='')
        print(" -> ".join([f"({r},{c})" for r, c in self.path_history]))
        print("=" * 50)


def run_graphical_demo(agent, grid_size: int = 4, is_slippery: bool = True,
                       wind_strength: str = 'medium', num_episodes: int = 3,
                       step_delay: float = 0.5):
    """
    å›¾å½¢ç•Œé¢æ¼”ç¤ºï¼ˆå°äººåŠ¨ç”»ï¼‰
    
    Args:
        agent: è®­ç»ƒå¥½çš„ Agent
        grid_size: ç½‘æ ¼å¤§å°
        is_slippery: ç¯å¢ƒæ˜¯å¦éšæœº
        wind_strength: é£åŠ›å¼ºåº¦ 'strong'/'medium'/'weak'/'none'
        num_episodes: æ¼”ç¤ºå›åˆæ•°
        step_delay: æ¯æ­¥å»¶è¿Ÿ
    """
    # é£åŠ›æ˜ å°„
    wind_map = {
        'strong': 0.333,   # å¼ºé£ - 1/3æ¦‚ç‡åç§»
        'medium': 0.2,     # ä¸­é£ - 20%æ¦‚ç‡åç§»
        'weak': 0.1,       # å¼±é£ - 10%æ¦‚ç‡åç§»
        'none': 0.0        # æ— é£
    }
    wind_prob = wind_map.get(wind_strength, 0.2)
    
    map_name = f'{grid_size}x{grid_size}'
    
    # åˆ›å»ºå¸¦å›¾å½¢æ¸²æŸ“çš„ç¯å¢ƒ
    env = gym.make('FrozenLake-v1', 
                   map_name=map_name,
                   is_slippery=is_slippery,
                   render_mode='human')
    
    print(f"\n{'='*50}")
    print("å›¾å½¢ç•Œé¢æ¼”ç¤º")
    print(f"{'='*50}")
    print(f"åœ°å›¾: {map_name}")
    print(f"ç¯å¢ƒéšæœºæ€§: {'å¼€å¯' if is_slippery else 'å…³é—­'}")
    print(f"é£åŠ›å¼ºåº¦: {wind_strength} (åç§»æ¦‚ç‡: {wind_prob*100:.0f}%)")
    print(f"æ¼”ç¤ºå›åˆ: {num_episodes}")
    print(f"{'='*50}")
    
    success_count = 0
    
    for episode in range(num_episodes):
        print(f"\n>>> å›åˆ {episode + 1}/{num_episodes}")
        
        state, _ = env.reset()
        env.render()
        time.sleep(step_delay)
        
        total_steps = 0
        wind_shifts = 0
        
        for step in range(100):
            # Agent é€‰æ‹©åŠ¨ä½œ
            intended_action = agent.select_action(state, training=False)
            
            # æ¨¡æ‹Ÿé£åŠ›åç§»ï¼ˆåœ¨æˆ‘ä»¬è‡ªå·±çš„é€»è¾‘ä¸­ï¼‰
            if is_slippery and wind_prob > 0:
                rand = random.random()
                if rand < 1 - 2 * wind_prob:
                    actual_action = intended_action
                elif rand < 1 - wind_prob:
                    actual_action = (intended_action - 1) % 4
                    wind_shifts += 1
                else:
                    actual_action = (intended_action + 1) % 4
                    wind_shifts += 1
            else:
                actual_action = intended_action
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(actual_action)
            done = terminated or truncated
            
            env.render()
            time.sleep(step_delay)
            
            total_steps += 1
            state = next_state
            
            if done:
                if reward > 0:
                    print(f"    ğŸ‰ æˆåŠŸ! æ­¥æ•°: {total_steps}, é£å: {wind_shifts}æ¬¡")
                    success_count += 1
                else:
                    print(f"    ğŸ’€ å¤±è´¥! æ­¥æ•°: {total_steps}, é£å: {wind_shifts}æ¬¡")
                time.sleep(1)
                break
        
        if not done:
            print(f"    â° è¶…æ—¶!")
    
    print(f"\n{'='*50}")
    print(f"æ¼”ç¤ºå®Œæˆ! æˆåŠŸç‡: {success_count}/{num_episodes}")
    print(f"{'='*50}")
    
    env.close()


def run_terminal_demo(agent, env_wrapper: FrozenLakeWrapper,
                      wind_strength: str = 'medium',
                      max_steps: int = 100,
                      step_delay: float = 0.3,
                      verbose: bool = True):
    """
    ç»ˆç«¯æ–‡å­—æ¼”ç¤º
    """
    wind_map = {
        'strong': 0.333,
        'medium': 0.2,
        'weak': 0.1,
        'none': 0.0
    }
    wind_prob = wind_map.get(wind_strength, 0.2)
    
    controller = FlightDemoController(
        grid_size=env_wrapper.grid_size,
        is_stochastic=env_wrapper.is_slippery,
        wind_probability=wind_prob
    )
    controller.set_map(env_wrapper.desc)
    
    state, _ = env_wrapper.reset()
    start_row, start_col = env_wrapper.state_to_coord(state)
    controller.reset(start_row, start_col)
    
    print(f"\n{'='*50}")
    print("ç»ˆç«¯æ–‡å­—æ¼”ç¤º")
    print(f"{'='*50}")
    print(f"ç½‘æ ¼: {env_wrapper.grid_size}x{env_wrapper.grid_size}")
    print(f"é£åŠ›: {wind_strength} (åç§»æ¦‚ç‡: {wind_prob*100:.0f}%)")
    print(f"{'='*50}")
    
    controller.print_current_state()
    time.sleep(step_delay)
    
    for step in range(max_steps):
        current_state = controller.get_current_state()
        intended_action = controller.select_valid_action(agent, current_state)
        
        success, msg, actual = controller.execute_action(
            intended_action, 
            apply_wind=env_wrapper.is_slippery,
            verbose=verbose
        )
        
        controller.print_current_state()
        
        if controller.is_terminal():
            if controller.is_goal():
                print("\nğŸ‰ æˆåŠŸåˆ°è¾¾ç›®æ ‡!")
            else:
                print("\nğŸ’€ æ‰å…¥å†°çªŸçª¿!")
            break
        
        time.sleep(step_delay)
    
    controller.print_summary()
    return controller


def manual_demo():
    """æ‰‹åŠ¨æ§åˆ¶æ¼”ç¤º"""
    print("\n" + "=" * 50)
    print("æ‰‹åŠ¨æ§åˆ¶æ¨¡å¼")
    print("=" * 50)
    
    # é…ç½®
    print("\nåœ°å›¾å¤§å°: 1=4x4, 2=8x8")
    size = input("é€‰æ‹© (é»˜è®¤1): ").strip()
    grid_size = 8 if size == '2' else 4
    
    print("\né£åŠ›å¼ºåº¦: 1=æ— é£, 2=å¼±é£, 3=ä¸­é£, 4=å¼ºé£")
    wind = input("é€‰æ‹© (é»˜è®¤3): ").strip()
    wind_map = {'1': 'none', '2': 'weak', '3': 'medium', '4': 'strong'}
    wind_strength = wind_map.get(wind, 'medium')
    
    wind_prob_map = {'none': 0, 'weak': 0.1, 'medium': 0.2, 'strong': 0.333}
    wind_prob = wind_prob_map[wind_strength]
    
    env = FrozenLakeWrapper(map_size=f'{grid_size}x{grid_size}', is_slippery=wind_prob > 0)
    controller = FlightDemoController(grid_size, wind_prob > 0, wind_prob)
    controller.set_map(env.desc)
    
    state, _ = env.reset()
    row, col = env.state_to_coord(state)
    controller.reset(row, col)
    
    print("\næ“ä½œ: W=å‰, S=å, A=å·¦, D=å³, Q=é€€å‡º")
    controller.print_current_state()
    
    action_map = {'w': 1, 's': 3, 'a': 0, 'd': 2}
    
    while not controller.is_terminal():
        cmd = input("åŠ¨ä½œ: ").strip().lower()
        if cmd == 'q':
            break
        if cmd not in action_map:
            print("æ— æ•ˆ! è¯·ç”¨ W/A/S/D")
            continue
        
        controller.execute_action(action_map[cmd], apply_wind=wind_prob > 0)
        controller.print_current_state()
    
    controller.print_summary()
    env.close()


def main():
    """ä¸»å…¥å£"""
    print("\n" + "=" * 50)
    print("ğŸš æ— äººæœºé£è¡Œæ¼”ç¤ºç³»ç»Ÿ")
    print("=" * 50)
    print("\næ¼”ç¤ºæ¨¡å¼:")
    print("  1. å›¾å½¢ç•Œé¢æ¼”ç¤º (å°äººåŠ¨ç”»)")
    print("  2. ç»ˆç«¯æ–‡å­—æ¼”ç¤º")
    print("  3. æ‰‹åŠ¨æ§åˆ¶")
    print("  4. å¿«é€Ÿéšæœºæµ‹è¯•")
    
    mode = input("\né€‰æ‹©æ¨¡å¼ (1/2/3/4): ").strip()
    
    if mode in ['1', '2']:
        # é€‰æ‹©æ¨¡å‹
        print("\né€‰æ‹©æ¨¡å‹:")
        print("  1. Q-Learning")
        print("  2. DQN")
        print("  3. Double DQN")
        model_choice = input("é€‰æ‹© (é»˜è®¤1): ").strip()
        
        model_paths = {
            '1': ('models/q_learning.npz', 'q_learning'),
            '2': ('models/dqn.pth', 'dqn'),
            '3': ('models/ddqn.pth', 'dqn')
        }
        
        path, model_type = model_paths.get(model_choice, model_paths['1'])
        
        if not os.path.exists(path):
            print(f"\nâŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            print("è¯·å…ˆè¿è¡Œ python train.py è®­ç»ƒæ¨¡å‹")
            return
        
        # åŠ è½½æ¨¡å‹
        print(f"\nåŠ è½½æ¨¡å‹: {path}")
        if model_type == 'q_learning':
            agent = QLearningAgent(16, 4)  # 4x4
            agent.load(path)
        else:
            agent = DQNAgent(16, 4)
            agent.load(path)
        
        # é€‰æ‹©é£åŠ›
        print("\né£åŠ›å¼ºåº¦:")
        print("  1. æ— é£ (ç¡®å®šæ€§)")
        print("  2. å¼±é£ (10%åç§»)")
        print("  3. ä¸­é£ (20%åç§»)")
        print("  4. å¼ºé£ (33%åç§»)")
        wind_choice = input("é€‰æ‹© (é»˜è®¤3): ").strip()
        wind_map = {'1': 'none', '2': 'weak', '3': 'medium', '4': 'strong'}
        wind_strength = wind_map.get(wind_choice, 'medium')
        
        is_slippery = wind_strength != 'none'
        
        if mode == '1':
            # å›¾å½¢æ¼”ç¤º
            print("\næ¼”ç¤ºå›åˆæ•°:")
            episodes = input("è¾“å…¥æ•°é‡ (é»˜è®¤3): ").strip()
            num_episodes = int(episodes) if episodes.isdigit() else 3
            
            run_graphical_demo(
                agent, 
                grid_size=4,
                is_slippery=is_slippery,
                wind_strength=wind_strength,
                num_episodes=num_episodes,
                step_delay=0.5
            )
        else:
            # ç»ˆç«¯æ¼”ç¤º
            env = FrozenLakeWrapper(map_size='4x4', is_slippery=is_slippery)
            run_terminal_demo(agent, env, wind_strength=wind_strength)
            env.close()
    
    elif mode == '3':
        manual_demo()
    
    else:
        # éšæœºæµ‹è¯•
        print("\nå¿«é€Ÿéšæœºæµ‹è¯•...")
        agent = QLearningAgent(16, 4)
        agent.epsilon = 1.0
        
        run_graphical_demo(
            agent,
            grid_size=4,
            is_slippery=True,
            wind_strength='weak',
            num_episodes=2,
            step_delay=0.3
        )


if __name__ == '__main__':
    main()
