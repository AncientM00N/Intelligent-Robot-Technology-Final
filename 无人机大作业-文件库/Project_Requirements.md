# 智能机器人技术大作业：深度强化学习无人机飞行控制

## 1. 项目背景与目标

本项目旨在利用深度强化学习（Deep Reinforcement Learning, DRL）算法，在 OpenAI Gym 环境下解决无人机（Agent）的路径规划与飞行控制问题。

* **核心场景**：`FrozenLake`（冰湖环境），模拟无人机从起点(S)飞往终点(G)，需避开冰窟窿(H)。
* **核心挑战**：环境存在风力干扰（Stochastic 模式），无人机可能发生不可控漂移。

## 2. 环境设置 (Environment Setup)

* **基础库**：OpenAI `gym`。
* **地图配置**：
  * **基础地图**：4x4 网格。
  * **进阶地图**：8x8 网格（需定制 `map_name='8x8'`）。
  * **定制地图**：需支持自定义地图数组（如 `desc=custom_map`）。
* **运行模式**：
  1. **Deterministic (确定性模式)**：`is_slippery=False`。环境完全服从动作指令。
  2. **Stochastic (随机模式)**：`is_slippery=True`。执行动作后，Agent 有一定概率向其他方向漂移（模拟风力/打滑）。

## 3. 任务清单 (Tasks)

### 核心任务

1. **仿真训练**：在 Stochastic 和 Deterministic 两种模式下完成模型训练，生成策略。
2. **路径生成**：输出并保存无人机的飞行路径。
3. **实机/模拟飞行演示**：
   * 使用 Python 脚本控制（模拟）无人机飞越冰湖。
   * **必须实现**：电脑端实时显示飞机当前所在的网格位置。
   * **可视化**：通过 `env.render()` 或 GUI 展示结果。

### 挑战任务 (加分项)

1. **复杂地图**：使用 8x8 或加权地图（寻找最短安全路径）。
2. **算法升级**：除基础 Q-Learning 外，实现 DQN (Deep Q-Network), Double DQN (DDQN) 等算法。
3. **实机逻辑**：需包含特定的边界检测与位置判断逻辑（见下文代码逻辑）。

## 4. 特殊逻辑代码：实机飞行演示与边界检测

在完成 Stochastic 模式下的实机飞行演示时，必须包含以下“位置判断与边界保护”逻辑。这模拟了无人机在物理空间中飞行时，防止飞出边界的底层控制。

**逻辑描述**：

* 维护变量：`forward_steps`, `right_steps`, `left_steps`, `back_steps` (初始根据起点位置设定，如 4x4 地图中起点在左上角，则前/右为3，左/后为0)。
* 在执行动作前，先判断动作是否会导致撞墙（飞出边界）。
* 如果动作无效（撞墙），则重新采样或修正，直到动作有效才执行 `env.step()`。
* 在 Stochastic 模式下，动作执行后需通过模拟随机性（`random.choice`）来体现环境干扰。

## 5. 评分标准 (Total: 100 pts)

1. **仿真模型训练 (20分)**：代码运行成功，模型收敛。
2. **实机飞行演示 (20分)**：参照逻辑完成演示。
3. **实时位置显示 (20分)**：演示过程中实时反馈位置。
4. **结果分析 (20分)**：深入分析算法优缺点（如 Q-Learning vs DQN 在过度估计上的差异）。
5. **报告质量 (20分)**：格式整洁，思路清晰。

## 6. 算法参考

* **Q-Learning**: 基础表格型 RL。
* **DQN**: 结合神经网络与经验回放 (Experience Replay)。
* **Nature DQN**: 使用 Target Network 解决相关性问题。
* **Double DQN (DDQN)**: 解耦动作选择与价值评估，解决 Q 值过高估计问题。
